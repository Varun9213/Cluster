# -*- coding: utf-8 -*-
"""DEC_Clustering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g6DV5GDiLmjGbTLKlQ9ptjKePihd1PWK
"""

import numpy as np
import pandas as pd

import tensorflow as tf
import tensorflow.keras.backend as K
import tensorflow.keras.layers as L
from tensorflow.keras.optimizers import SGD, Adam
from tensorflow.keras.models import Model


from sklearn.datasets import load_iris, load_digits
from sklearn.cluster import KMeans
from sklearn import metrics

def autoencoder(DIM, act='relu', initializer='glorot_uniform'):

  """
  DIM : Number of units in each layer where DIM[0] corresponds to input layer and DIM[-1] corresponds to encoder output and decoder input
  act : Activation Function

  """
  inp = L.Input(shape=(DIM[0],), name='input_')
  e = inp

  #Encoding Layers
  for i in range(len(DIM)-2):
    e = L.Dense(DIM[i+1], act, kernel_initializer=initializer, name="encoding_layer_%i"%(i))(e)

  e = L.Dense(DIM[-1], kernel_initializer=initializer, name='encoder_output')(e)
  encoder_out = e 

  d = e
  #Decoding Layers 
  for i in range(len(DIM)-2, 0, -1):
    d = L.Dense(DIM[i], act, kernel_initializer=initializer, name="decoding_layer_%i"%(i))(d)

  decoder_out = L.Dense(DIM[0], kernel_initializer=initializer, name='decoder_ouput')(d)

  encoder = Model(inp,encoder_out,name="Encoder")
  autoencoder = Model(inp,decoder_out,name="Autoencoder")

  return autoencoder, encoder

class ClusteringLayer(tf.keras.layers.Layer):

  def __init__(self,nclusters,weights=None,alpha=1.0,**kwargs):
    if 'input_shape' not in kwargs and 'input_dim' in kwargs:
      kwargs['input_shape'] = (kwargs.pop('input_dim'),)
    super().__init__(**kwargs)
    self.nclusters = nclusters
    self.init_weights = weights
    self.alpha = alpha

  def build(self, input_shape):
    self.inp_dim = input_shape[1]
    self.clusters = self.add_weight(shape=(self.nclusters,self.inp_dim),initializer='glorot_uniform', name='clusters')
    if self.init_weights is not None:
      self.set_weights(self.init_weights)
      self.built = True

  def call(self, inputs, **kwargs):
    q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))
    q = K.pow(q,(self.alpha + 1.0) / 2.0)
    q = K.transpose(K.transpose(q) / K.sum(q, axis=1))
    return q

class DEC(object):

  def __init__(self,DIM,nclusters=10,alpha=1.0,initializer='glorot_uniform'):
    self.DIM = DIM
    self.input_dim = DIM[0]
    self.nclusters = nclusters
    self.alpha = alpha
    self.autoencoder, self.encoder = autoencoder(DIM, initializer=initializer)

    clustering_layer = ClusteringLayer(self.nclusters, name='cluster_layer')(self.encoder.output)
    self.model = Model(inputs=self.encoder.input, outputs=clustering_layer)

  def train_autoencoder(self, x, optimizer='adam', epochs=100, batch_size=256, dir = None):

    self.autoencoder.compile(optimizer=optimizer, loss='mse')
    hist = self.autoencoder.fit(x,x,epochs=epochs,batch_size=batch_size,verbose=0)
    print("loss : %.5f"%hist.history["loss"][-1])
    self.autoencoder.save_weights(dir + "/autoencoder_weights.h5")
    print("Weights saved to %s/autoencoder_weights.h5" %(dir))


  def target_distribution(self,q):
    weight = q ** 2 / q.sum(0)
    return (weight.T / weight.sum(1)).T

  def compile(self, optimizer='sgd', loss='kld'):
    self.model.compile(optimizer=optimizer, loss=loss)

  def predict(self, x):
    q = self.model.predict(x)
    return np.argmax(q,axis=1)

  def fit(self, x, y=None, epochs=200, batch_size=256, dir = None):

    print('Initializing cluster centers with k-means.........')
    km = KMeans(n_clusters=self.nclusters, n_init=5)
    y_pred = km.fit_predict(self.encoder.predict(x))
    self.model.summary()
    self.model.get_layer('cluster_layer').set_weights([km.cluster_centers_])
    loss = 0
    indx = 0
    idx_arr = np.arange(x.shape[0])

    for i in range(epochs):

      q = self.model.predict(x)
      p = self.target_distribution(q)
      idx = idx_arr[indx*batch_size:min((indx+1) * batch_size, x.shape[0])]
      loss = self.model.train_on_batch(x[idx],p[idx])
      indx = indx + 1 if (indx + 1) * batch_size <= x.shape[0] else 0
    print('Loss : %.5f'%loss)
    print('saving model to:', dir + "/DEC_model.h5")
    self.model.save_weights(dir + "/DEC_model.h5")